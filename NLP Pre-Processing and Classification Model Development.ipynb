{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_json('train.data.jsonl', orient='values', lines=True, encoding='utf-8')\n",
    "dev = pd.read_json('dev.data.jsonl', orient='values', lines=True, encoding='utf-8')\n",
    "\n",
    "y_train = pd.read_json('train.label.json',orient='index', convert_dates = False, encoding='utf-8',convert_axes=False)\n",
    "y_dev = pd.read_json('dev.label.json',orient='index', convert_dates = False, encoding='utf-8',convert_axes=False)\n",
    "\n",
    "test = pd.read_json('test.data.jsonl', orient='values', lines=True, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check if first tweet ID match label\n",
    "sum([0 if y==x else 1 for x,y in zip(df.iloc[:,0].apply(lambda x: x['id_str']), y_train.index)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'contributors': None,\n",
       " 'truncated': False,\n",
       " 'text': 'How to respond to the murderous attack on Charlie Hebdo? Every newspaper in the free world should print this. http://t.co/sC2ot63F6j',\n",
       " 'in_reply_to_status_id': None,\n",
       " 'id': 552800070199148544,\n",
       " 'favorite_count': 77,\n",
       " 'source': '<a href=\"http://www.apple.com\" rel=\"nofollow\">iOS</a>',\n",
       " 'retweeted': False,\n",
       " 'coordinates': None,\n",
       " 'entities': {'symbols': [],\n",
       "  'user_mentions': [],\n",
       "  'hashtags': [],\n",
       "  'urls': [],\n",
       "  'media': [{'expanded_url': 'http://twitter.com/Heresy_Corner/status/552800070199148544/photo/1',\n",
       "    'display_url': 'pic.twitter.com/sC2ot63F6j',\n",
       "    'url': 'http://t.co/sC2ot63F6j',\n",
       "    'media_url_https': 'https://pbs.twimg.com/media/B6vwvCVIQAASBJx.jpg',\n",
       "    'id_str': '552800070153027584',\n",
       "    'sizes': {'small': {'h': 408, 'resize': 'fit', 'w': 340},\n",
       "     'large': {'h': 472, 'resize': 'fit', 'w': 393},\n",
       "     'medium': {'h': 472, 'resize': 'fit', 'w': 393},\n",
       "     'thumb': {'h': 150, 'resize': 'crop', 'w': 150}},\n",
       "    'indices': [110, 132],\n",
       "    'type': 'photo',\n",
       "    'id': 552800070153027584,\n",
       "    'media_url': 'http://pbs.twimg.com/media/B6vwvCVIQAASBJx.jpg'}]},\n",
       " 'in_reply_to_screen_name': None,\n",
       " 'id_str': '552800070199148544',\n",
       " 'retweet_count': 228,\n",
       " 'in_reply_to_user_id': None,\n",
       " 'favorited': False,\n",
       " 'user': {'follow_request_sent': False,\n",
       "  'profile_use_background_image': True,\n",
       "  'profile_text_color': '0F0103',\n",
       "  'default_profile_image': False,\n",
       "  'id': 60296618,\n",
       "  'profile_background_image_url_https': 'https://pbs.twimg.com/profile_background_images/98945974/heresy-warhol.jpg',\n",
       "  'verified': False,\n",
       "  'profile_location': None,\n",
       "  'profile_image_url_https': 'https://pbs.twimg.com/profile_images/556109555990401025/rsRm-g4o_normal.jpeg',\n",
       "  'profile_sidebar_fill_color': 'BDFFDB',\n",
       "  'entities': {'url': {'urls': [{'url': 'http://t.co/3nF8nkFdkC',\n",
       "      'indices': [0, 22],\n",
       "      'expanded_url': 'http://heresycorner.blogspot.com',\n",
       "      'display_url': 'heresycorner.blogspot.com'}]},\n",
       "   'description': {'urls': []}},\n",
       "  'followers_count': 4144,\n",
       "  'profile_sidebar_border_color': 'D3D2CF',\n",
       "  'id_str': '60296618',\n",
       "  'profile_background_color': 'D6CBAB',\n",
       "  'listed_count': 185,\n",
       "  'is_translation_enabled': False,\n",
       "  'utc_offset': 0,\n",
       "  'statuses_count': 27923,\n",
       "  'description': 'Aka the Heresiarch; \\r\\nA sceptic, not a Skeptic',\n",
       "  'friends_count': 414,\n",
       "  'location': 'Cambridge',\n",
       "  'profile_link_color': '911F50',\n",
       "  'profile_image_url': 'http://pbs.twimg.com/profile_images/556109555990401025/rsRm-g4o_normal.jpeg',\n",
       "  'following': False,\n",
       "  'geo_enabled': False,\n",
       "  'profile_banner_url': 'https://pbs.twimg.com/profile_banners/60296618/1357129427',\n",
       "  'profile_background_image_url': 'http://pbs.twimg.com/profile_background_images/98945974/heresy-warhol.jpg',\n",
       "  'name': 'Nelson Jones',\n",
       "  'lang': 'en',\n",
       "  'profile_background_tile': True,\n",
       "  'favourites_count': 500,\n",
       "  'screen_name': 'Heresy_Corner',\n",
       "  'notifications': False,\n",
       "  'url': 'http://t.co/3nF8nkFdkC',\n",
       "  'created_at': 'Sun Jul 26 12:40:25 +0000 2009',\n",
       "  'contributors_enabled': False,\n",
       "  'time_zone': 'London',\n",
       "  'protected': False,\n",
       "  'default_profile': False,\n",
       "  'is_translator': False},\n",
       " 'geo': None,\n",
       " 'in_reply_to_user_id_str': None,\n",
       " 'possibly_sensitive': False,\n",
       " 'lang': 'en',\n",
       " 'created_at': 'Wed Jan 07 12:13:01 +0000 2015',\n",
       " 'in_reply_to_status_id_str': None,\n",
       " 'place': None,\n",
       " 'extended_entities': {'media': [{'expanded_url': 'http://twitter.com/Heresy_Corner/status/552800070199148544/photo/1',\n",
       "    'display_url': 'pic.twitter.com/sC2ot63F6j',\n",
       "    'url': 'http://t.co/sC2ot63F6j',\n",
       "    'media_url_https': 'https://pbs.twimg.com/media/B6vwvCVIQAASBJx.jpg',\n",
       "    'id_str': '552800070153027584',\n",
       "    'sizes': {'small': {'h': 408, 'resize': 'fit', 'w': 340},\n",
       "     'large': {'h': 472, 'resize': 'fit', 'w': 393},\n",
       "     'medium': {'h': 472, 'resize': 'fit', 'w': 393},\n",
       "     'thumb': {'h': 150, 'resize': 'crop', 'w': 150}},\n",
       "    'indices': [110, 132],\n",
       "    'type': 'photo',\n",
       "    'id': 552800070153027584,\n",
       "    'media_url': 'http://pbs.twimg.com/media/B6vwvCVIQAASBJx.jpg'}]}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>index</th>\n",
       "      <th>lab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>552800070199148544</th>\n",
       "      <td>non-rumour</td>\n",
       "      <td>552800070199148544</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544388259359387648</th>\n",
       "      <td>non-rumour</td>\n",
       "      <td>544388259359387648</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552805970536333314</th>\n",
       "      <td>non-rumour</td>\n",
       "      <td>552805970536333314</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525071376084791297</th>\n",
       "      <td>rumour</td>\n",
       "      <td>525071376084791297</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498355319979143168</th>\n",
       "      <td>non-rumour</td>\n",
       "      <td>498355319979143168</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524959027516932096</th>\n",
       "      <td>rumour</td>\n",
       "      <td>524959027516932096</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524940940721418240</th>\n",
       "      <td>non-rumour</td>\n",
       "      <td>524940940721418240</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580331453889708032</th>\n",
       "      <td>rumour</td>\n",
       "      <td>580331453889708032</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552820384039706624</th>\n",
       "      <td>non-rumour</td>\n",
       "      <td>552820384039706624</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499426471300329472</th>\n",
       "      <td>non-rumour</td>\n",
       "      <td>499426471300329472</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4641 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         label               index  lab\n",
       "552800070199148544  non-rumour  552800070199148544    0\n",
       "544388259359387648  non-rumour  544388259359387648    0\n",
       "552805970536333314  non-rumour  552805970536333314    0\n",
       "525071376084791297      rumour  525071376084791297    1\n",
       "498355319979143168  non-rumour  498355319979143168    0\n",
       "...                        ...                 ...  ...\n",
       "524959027516932096      rumour  524959027516932096    1\n",
       "524940940721418240  non-rumour  524940940721418240    0\n",
       "580331453889708032      rumour  580331453889708032    1\n",
       "552820384039706624  non-rumour  552820384039706624    0\n",
       "499426471300329472  non-rumour  499426471300329472    0\n",
       "\n",
       "[4641 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.columns = ['label']\n",
    "y_train['index'] = y_train.index\n",
    "y_train['lab'] = [1 if i=='rumour' else 0 for i in y_train.label]\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>index</th>\n",
       "      <th>lab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>553588913747808256</th>\n",
       "      <td>rumour</td>\n",
       "      <td>553588913747808256</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524949003834634240</th>\n",
       "      <td>non-rumour</td>\n",
       "      <td>524949003834634240</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553221281181859841</th>\n",
       "      <td>non-rumour</td>\n",
       "      <td>553221281181859841</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580322346508124160</th>\n",
       "      <td>rumour</td>\n",
       "      <td>580322346508124160</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544307417677189121</th>\n",
       "      <td>rumour</td>\n",
       "      <td>544307417677189121</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525025279803424768</th>\n",
       "      <td>rumour</td>\n",
       "      <td>525025279803424768</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552784600502915072</th>\n",
       "      <td>non-rumour</td>\n",
       "      <td>552784600502915072</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499696525808001024</th>\n",
       "      <td>non-rumour</td>\n",
       "      <td>499696525808001024</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580320612155060224</th>\n",
       "      <td>rumour</td>\n",
       "      <td>580320612155060224</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553218279557582849</th>\n",
       "      <td>non-rumour</td>\n",
       "      <td>553218279557582849</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>580 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         label               index  lab\n",
       "553588913747808256      rumour  553588913747808256    1\n",
       "524949003834634240  non-rumour  524949003834634240    0\n",
       "553221281181859841  non-rumour  553221281181859841    0\n",
       "580322346508124160      rumour  580322346508124160    1\n",
       "544307417677189121      rumour  544307417677189121    1\n",
       "...                        ...                 ...  ...\n",
       "525025279803424768      rumour  525025279803424768    1\n",
       "552784600502915072  non-rumour  552784600502915072    0\n",
       "499696525808001024  non-rumour  499696525808001024    0\n",
       "580320612155060224      rumour  580320612155060224    1\n",
       "553218279557582849  non-rumour  553218279557582849    0\n",
       "\n",
       "[580 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_dev.columns = ['label']\n",
    "y_dev['index'] = y_dev.index\n",
    "y_dev['lab'] = [1 if i=='rumour' else 0 for i in y_dev.label]\n",
    "y_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph building function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class node(object):\n",
    "    \"\"\"\n",
    "    Tree data structure.\n",
    "    \"\"\"\n",
    "    def __init__(self, value, datas, children = []):\n",
    "        \"\"\"\n",
    "        Constructor. Contains:\n",
    "            value as id_str\n",
    "            datas as tweet dictionary\n",
    "            children as tweet responses\n",
    "        \"\"\"\n",
    "        self.value = value\n",
    "        self.children = children\n",
    "        self.datas = datas\n",
    "\n",
    "    def printed(self, level=0):\n",
    "        \"\"\"\n",
    "        Function to visualize the tree\n",
    "        \"\"\"\n",
    "        print(\"\\t\"*level+self.value+\"\\n\")\n",
    "        if len(self.children)>0:\n",
    "            for child in self.children:\n",
    "                child.printed(level+1)\n",
    "\n",
    "                \n",
    "def bfs(value, root):\n",
    "    \"\"\"\n",
    "    Breadth first search to look for a node in a tree given its id_str. \n",
    "    Start from the root of a given tree.\n",
    "    \"\"\"\n",
    "    queue = []\n",
    "    queue.append(root)\n",
    "    while len(queue) > 0:\n",
    "        current = queue.pop(0)\n",
    "        if current.value == value:\n",
    "            return current\n",
    "        for c in current.children:\n",
    "            queue.append(c)\n",
    "    return None\n",
    "\n",
    "def preOrder(node, by_deep):\n",
    "    \"\"\"\n",
    "    Flatten a tree using pre-orden algorithm.\n",
    "    \"\"\"\n",
    "    text=node.datas['text']\n",
    "    #node.children.sort(key= lambda x: datetime.strftime(datetime.strptime(x.datas['created_at'],'%a %b %d %H:%M:%S +0000 %Y'), '%Y-%m-%d %H:%M:%S'), reverse= True)\n",
    "    if by_deep:\n",
    "        node.children.sort(key=lambda x: len(x.children), reverse=True)\n",
    "    else:\n",
    "        node.children.sort(key= lambda x: x.datas['favorite_count'], reverse= True)\n",
    "    \n",
    "    for child in node.children:\n",
    "        text += preOrder(child, by_deep)\n",
    "    return ' ' + text\n",
    "\n",
    "def getGraph(df,k):  \n",
    "    \"\"\"\n",
    "    Build a tree given a list of id_str and replay to id_str. \n",
    "    Its iterates through the tree until all nodes in the list have been added.\n",
    "    \"\"\"\n",
    "    root = node(df.iloc[k,0]['id_str'], datas = df.iloc[k,0])\n",
    "    queue = [i for i in df.iloc[k,1:] if i is not None]\n",
    "\n",
    "    while queue:\n",
    "        current = queue.pop(0)\n",
    "        parent = bfs(current['in_reply_to_status_id_str'], root)\n",
    "        if parent is not None:\n",
    "            parent.children = parent.children + [node(current['id_str'], datas = current)]\n",
    "        elif current['in_reply_to_status_id_str'] in [i['id_str'] for i in queue]:\n",
    "            queue.append(current)\n",
    "        else:\n",
    "            #creates an empty node and assumes its connects directly to root\n",
    "            root.children = root.children + [node(current['in_reply_to_status_id_str'], datas = {'favorite_count':0, 'text':' ', 'id_str': current['in_reply_to_status_id_str']})]\n",
    "            queue.append(current)\n",
    "    \n",
    "    return root\n",
    "\n",
    "def simpleConcat(df,k, n=None):\n",
    "    \"\"\"\n",
    "    Concatenate event tweets text in the order of the original dataset\n",
    "    Takes as arguments de dataframe, event row and the maximum number of tweet to concatenate.\n",
    "    \"\"\"\n",
    "    text = [i['text'] for i in df.iloc[k,:] if i is not None]\n",
    "    return ' '.join(text[:n])\n",
    "\n",
    "def sortByDate(df,k,n=None):\n",
    "    \"\"\"\n",
    "    Concatenate event tweets text ordered by time stamp.\n",
    "    Takes as arguments de dataframe, event row and the maximum number of tweet to concatenate.\n",
    "    \"\"\"\n",
    "    from datetime import datetime\n",
    "    twt = pd.DataFrame([t for t in df.iloc[k,:] if t is not None])\n",
    "    twt['created_at'] = twt.created_at.apply(lambda x: datetime.strftime(datetime.strptime(x,'%a %b %d %H:%M:%S +0000 %Y'), '%Y-%m-%d %H:%M:%S')) \n",
    "    twt = twt.sort_values(by='created_at')\n",
    "    \n",
    "    return ' '.join(twt[:n].text)\n",
    "\n",
    "def deepMetric(node):\n",
    "    \"\"\"\n",
    "    Takes as argument a root node of a tree data structure.\n",
    "    Returns the maximum depth or the height of a tree and the number of nodes.\n",
    "    \"\"\"\n",
    "    maxDeep = 0\n",
    "    n_nodes = 1\n",
    "    for c in node.children:\n",
    "        current, n_n = deepMetric(c)\n",
    "        n_nodes += n_n\n",
    "        if maxDeep < current:\n",
    "            maxDeep = current\n",
    "    return maxDeep + 1, n_nodes\n",
    "\n",
    "def ave_deep(node):\n",
    "    \"\"\"\n",
    "    Takes as argument a root node of a tree data structure.\n",
    "    Takes the deepMetric outputs and calculate the depth normalized by the total number of nodes.\n",
    "    \"\"\"\n",
    "    max_deep, n_nodes = deepMetric(node)\n",
    "    return max_deep/n_nodes\n",
    "    \n",
    "def low2high(node):\n",
    "    \"\"\"\n",
    "    Takes as argument a root node of a tree data structure.\n",
    "    Returns the number of directed edges were user source is less popular than the target and the number of edges.\n",
    "    \"\"\"\n",
    "    n_edges = 1\n",
    "    n_low2high = 0\n",
    "    for c in node.children:\n",
    "        n_e, n_l2h = low2high(c)\n",
    "        n_edges += n_e\n",
    "        n_low2high += n_l2h\n",
    "        if 'user' in node.datas and 'user' in c.datas:\n",
    "            if node.datas['user']['followers_count'] < c.datas['user']['followers_count']:\n",
    "                n_low2high += 1\n",
    "            \n",
    "    return n_edges, n_low2high\n",
    "\n",
    "def lowDifussion(node):\n",
    "    \"\"\"\n",
    "    Takes as argument a root node of a tree data structure.\n",
    "    Calculate the ratio went a tweet goes from a low to high difussion node versus the total tweets.\n",
    "    It uses output from low2high function.\n",
    "    \"\"\"\n",
    "    edges, low = low2high(node)\n",
    "    return low/edges\n",
    "\n",
    "def leaf_count(node):\n",
    "    \"\"\"\n",
    "    Takes as argument a root node of a tree data structure.\n",
    "    Counts the number of leafs in a tree and the total number of nodes.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    total_c = 1\n",
    "    for c in node.children:\n",
    "        part_count, total = leaf_count(c)\n",
    "        count += part_count\n",
    "        total_c += total\n",
    "    if len(node.children) < 1:\n",
    "        count +=1\n",
    "    return count, total_c\n",
    "\n",
    "def isolated_node(node):\n",
    "    \"\"\"\n",
    "    Takes as argument a root node of a tree data structure.\n",
    "    Calcultes the isolation ratio, the number of tweets without a replay versus the totla number of tweets.\n",
    "    \"\"\"\n",
    "    leaf, total = leaf_count(node)\n",
    "    return leaf/total\n",
    "\n",
    "def nltk_sentiment(sentence):\n",
    "    \"\"\"\n",
    "    Calculates the sentiment of a text using Vader sentiment analyzer from NLTK.\n",
    "    \"\"\"\n",
    "    from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "    \n",
    "    nltk_sentiment = SentimentIntensityAnalyzer()\n",
    "    score = nltk_sentiment.polarity_scores(sentence)\n",
    "    return score['compound']\n",
    "\n",
    "def sentimentSeq(df,k):\n",
    "    \"\"\"\n",
    "    Calculates the sentiment of each text in a event.\n",
    "    \"\"\"\n",
    "    return [nltk_sentiment(t['text']) for t in df.iloc[k,:] if t is not None]\n",
    "\n",
    "def sentDate(df, k):\n",
    "    \"\"\"\n",
    "    It creates a list with the time stamp of each tweet in an event.\n",
    "    \"\"\"\n",
    "    from datetime import datetime\n",
    "    twt = pd.DataFrame([t for t in df.iloc[k,:] if t is not None])\n",
    "    twt['created_at'] = twt.created_at.apply(lambda x: datetime.strftime(datetime.strptime(x,'%a %b %d %H:%M:%S +0000 %Y'), '%Y-%m-%d %H:%M:%S')) \n",
    "    #twt = twt.sort_values(by='created_at')\n",
    "    return twt.created_at\n",
    "\n",
    "def favCalculation(df,k):\n",
    "    \"\"\"\n",
    "    Calculates the proportion of likes of the source tweet versus the average likes of the rest of the responses.\n",
    "    \"\"\"\n",
    "    SMOOTHING = 0.00001\n",
    "    fav = [i['favorite_count'] for i in df.iloc[k,1:] if i is not None]\n",
    "    mean = np.mean(fav) if len(fav) > 0 else 0\n",
    "    return np.log(1+df.iloc[k,0]['favorite_count']/(SMOOTHING + mean)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract and format data to pytorch consumible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "token = get_tokenizer('spacy', 'en_core_web_sm')\n",
    "\n",
    "\n",
    "def transformDF(df, label, tokenize=True, concatenation = True, concat_type = 'simple', n_tweets = None, order_by_deep = False):\n",
    "    \"\"\"\n",
    "    Transform and extract tweets data to a pandas dataframe, ready to export to json.\n",
    "    Takes the original Dataframe and the list of labels as arguments.\n",
    "    \n",
    "    Arguments: df, label, tokenize=True, concat_type = 'simple', n_tweets = -1, order_by_deep = False\n",
    "    \"\"\"\n",
    "    \n",
    "    index = []\n",
    "    string = []\n",
    "    source = []\n",
    "    has_tag = []\n",
    "    has_link = []\n",
    "    fav=[]\n",
    "    sent = []\n",
    "    sent_date = []\n",
    "    sent_total = []\n",
    "    deep = []\n",
    "    low=[]\n",
    "    isolation =[]\n",
    "    frifo_ratio = [] #friends to followers ratio\n",
    "\n",
    "    for k in range(df.shape[0]):\n",
    "        \n",
    "        index.append(df.iloc[k,0]['id_str'])\n",
    "        \n",
    "        if concatenation:\n",
    "            # text feature\n",
    "            if concat_type == 'simple':\n",
    "                concat = simpleConcat(df,k,n_tweets)\n",
    "            elif concat_type == 'date':\n",
    "                concat = sortByDate(df,k,n_tweets)\n",
    "            else:\n",
    "                concat = preOrder(getGraph(df,k), by_deep=order_by_deep)\n",
    "            if tokenize:\n",
    "                concat = token(concat)\n",
    "                concat = [i.lower() for i in concat]\n",
    "            string.append(concat)\n",
    "        else:\n",
    "            #Create sequenses\n",
    "            string.append([i['text'] for i in df.iloc[k,:] if i is not None])\n",
    "        \n",
    "        #graph features\n",
    "        tree = getGraph(df,k)\n",
    "        deep.append(ave_deep(tree))\n",
    "        low.append(lowDifussion(tree))\n",
    "        isolation.append(isolated_node(tree))\n",
    "        \n",
    "        #user features\n",
    "        source.append(df.iloc[k,0]['user']['name'])\n",
    "        frifo_ratio.append(df.iloc[k,0]['user']['friends_count']/(df.iloc[k,0]['user']['followers_count']+0.000001))\n",
    "        fav.append(favCalculation(df,k))\n",
    "        \n",
    "        #Special content features\n",
    "        has_tag.append(1 if '#' in df.iloc[k,0]['text'] else 0)\n",
    "        has_link.append(1 if 'http' in df.iloc[k,0]['text'] else 0)\n",
    "        \n",
    "        # Sentiment features\n",
    "        #sent.append(sentimentSeq(df,k))\n",
    "        #sent_date.append(sentDate(df,k))\n",
    "        #if tokenize:\n",
    "        #    sent_total.append(nltk_sentiment(\" \".join(concat)))\n",
    "        #else:\n",
    "            #sent_total.append(nltk_sentiment(concat))\n",
    "          \n",
    "        \n",
    "    return pd.DataFrame({'text': string,'source': source, 'ratio':frifo_ratio, 'tag': has_tag, 'isolation':isolation,\n",
    "                             #'sentiment': sent,'sent_date': sent_date,\n",
    "                         'link': has_link, 'fav': fav, 'deep': deep, 'low': low,\n",
    "                             'label': label, 'index': index})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = transformDF(df, y_train.lab)\n",
    "devel = transformDF(dev, y_dev.lab)\n",
    "test1 = transformDF(test, test.index) #use index to fill label\n",
    "\n",
    "#Export to json pytorchtext readable format\n",
    "\n",
    "#train1.to_json('train.json', orient = 'records', lines = True)\n",
    "#devel.to_json('dev.json', orient = 'records', lines = True)\n",
    "#test1.to_json('test.json', orient = 'records', lines = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure features MI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "def mutualInfoScores(df, y):\n",
    "    res = zip(df.columns,\n",
    "                   mutual_info_classif(df, y, discrete_features=False)\n",
    "                   )\n",
    "    sorted_scores = sorted(res, key= lambda k: k[1], reverse=True)\n",
    "    return sorted_scores\n",
    "\n",
    "\n",
    "#Get scores for each feature\n",
    "#ignore trackID, title and tags fetures\n",
    "scores = mutualInfoScores(train1.iloc[:,2:9], train1.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ratio', 0.07323665624978948),\n",
       " ('fav', 0.04196261288922831),\n",
       " ('low', 0.012337884923472764),\n",
       " ('link', 0.008326876136909744),\n",
       " ('tag', 0.004329119186485153),\n",
       " ('deep', 0.002757646357453458),\n",
       " ('isolation', 0.0017935966347604193)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 3058, 1: 1583}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{v: train1.label.tolist().count(v) for v in train1.label}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Develop multiple datasets for ssystematic exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m\n",
       "\u001b[0mtransformDF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mtokenize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mconcatenation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mconcat_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'simple'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mn_tweets\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0morder_by_deep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Transform and extract tweets data to a pandas dataframe, ready to export to json.\n",
       "Takes the original Dataframe and the list of labels as arguments.\n",
       "\n",
       "Arguments: df, label, tokenize=True, concat_type = 'simple', n_tweets = -1, order_by_deep = False\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\framo\\google drive\\0 mit\\0sem 1 2021\\nlp\\project\\<ipython-input-44-549f3189ed36>\n",
       "\u001b[1;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transformDF?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1.text.apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NO TOKEN\n",
    "\n",
    "##simpleconcat\n",
    "train1 = transformDF(df, y_train.lab, tokenize=False)\n",
    "dev1 = transformDF(dev, y_dev.lab, tokenize=False)\n",
    "train1.to_json('trainSimpleNT.json', orient = 'records', lines = True)\n",
    "dev1.to_json('devSimpleNT.json', orient = 'records', lines = True)\n",
    "\n",
    "##sort_by_date\n",
    "train2 = transformDF(df, y_train.lab, concat_type = 'date', tokenize=False)\n",
    "dev2 = transformDF(dev, y_dev.lab, concat_type = 'date', tokenize=False)\n",
    "train2.to_json('trainDateNT.json', orient = 'records', lines = True)\n",
    "dev2.to_json('devDateNT.json', orient = 'records', lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "##tree by replies\n",
    "train3 = transformDF(df, y_train.lab, concat_type = 'tree', order_by_deep = True, tokenize=False)\n",
    "dev3 = transformDF(dev, y_dev.lab, concat_type = 'tree', order_by_deep = True, tokenize=False)\n",
    "train3.to_json('trainDepthNT.json', orient = 'records', lines = True)\n",
    "dev3.to_json('devDepthNT.json', orient = 'records', lines = True)\n",
    "\n",
    "##tree by popularity\n",
    "train4 = transformDF(df, y_train.lab, concat_type = 'tree', order_by_deep = False, tokenize=False)\n",
    "dev4 = transformDF(dev, y_dev.lab, concat_type = 'tree', order_by_deep = False, tokenize=False)\n",
    "train4.to_json('trainLikeNT.json', orient = 'records', lines = True)\n",
    "dev4.to_json('devLikeNT.json', orient = 'records', lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZED DATA\n",
    "\n",
    "##simpleconcat\n",
    "train1 = transformDF(df, y_train.lab)\n",
    "dev1 = transformDF(dev, y_dev.lab)\n",
    "train1.to_json('trainSimple.json', orient = 'records', lines = True)\n",
    "dev1.to_json('devSimple.json', orient = 'records', lines = True)\n",
    "\n",
    "##sort_by_date\n",
    "train2 = transformDF(df, y_train.lab, concat_type = 'date')\n",
    "dev2 = transformDF(dev, y_dev.lab, concat_type = 'date')\n",
    "train2.to_json('trainDate.json', orient = 'records', lines = True)\n",
    "dev2.to_json('devDate.json', orient = 'records', lines = True)\n",
    "\n",
    "##tree by replies\n",
    "train3 = transformDF(df, y_train.lab, concat_type = 'tree', order_by_deep = True)\n",
    "dev3 = transformDF(dev, y_dev.lab, concat_type = 'tree', order_by_deep = True)\n",
    "train3.to_json('trainDepth.json', orient = 'records', lines = True)\n",
    "dev3.to_json('devDepth.json', orient = 'records', lines = True)\n",
    "\n",
    "##tree by popularity\n",
    "train4 = transformDF(df, y_train.lab, concat_type = 'tree', order_by_deep = False)\n",
    "dev4 = transformDF(dev, y_dev.lab, concat_type = 'tree', order_by_deep = False)\n",
    "train4.to_json('trainLike.json', orient = 'records', lines = True)\n",
    "dev4.to_json('devLike.json', orient = 'records', lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sequense DATA\n",
    "\n",
    "##simple sequence\n",
    "train5 = transformDF(df, y_train.lab, concatenation = False)\n",
    "dev5 = transformDF(dev, y_dev.lab, concatenation = False)\n",
    "train5.to_json('trainSeq.json', orient = 'records', lines = True)\n",
    "dev5.to_json('devSeq.json', orient = 'records', lines = True)\n",
    "\n",
    "## sequence by date\n",
    "train5 = transformDF(df, y_train.lab, concatenation = False)\n",
    "dev5 = transformDF(dev, y_dev.lab, concatenation = False)\n",
    "train5.to_json('trainSeq.json', orient = 'records', lines = True)\n",
    "dev5.to_json('devSeq.json', orient = 'records', lines = True)\n",
    "\n",
    "## sequence by tree\n",
    "train5 = transformDF(df, y_train.lab, concatenation = False)\n",
    "dev5 = transformDF(dev, y_dev.lab, concatenation = False)\n",
    "train5.to_json('trainSeq.json', orient = 'records', lines = True)\n",
    "dev5.to_json('devSeq.json', orient = 'records', lines = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BI LSTM RNN + simple concat + Additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.legacy import data\n",
    "from torchtext.legacy import datasets\n",
    "\n",
    "#TXT = data.Field(sequential=True,include_lengths=True, batch_first=True)#,dtype = torch.float)#tokenize = 'spacy',tokenizer_language = 'en_core_web_sm',include_lengths = True)\n",
    "#SOURCE = data.Field()\n",
    "#LABEL = data.Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.float)#,dtype = torch.float)\n",
    "TXT = data.Field(sequential=True, include_lengths=True)\n",
    "LABEL = data.Field(sequential=False, use_vocab=False)\n",
    "RATIO = data.Field(sequential=False, use_vocab=False, dtype=torch.float)\n",
    "INDEX = data.Field(sequential=False,use_vocab=False)\n",
    "ISO = data.Field(sequential=False, use_vocab=False,dtype=torch.float)\n",
    "LOW = data.Field(sequential=False, use_vocab=False, dtype=torch.float)\n",
    "DEEP = data.Field(sequential=False, use_vocab=False, dtype=torch.float)\n",
    "train_data, dev_data, test_data = data.TabularDataset.splits(\n",
    "                            path ='./',\n",
    "                            train = 'train.json',\n",
    "                            validation = 'dev.json',\n",
    "                            test = 'test.json',\n",
    "                            format = 'json',\n",
    "                            fields = {'text': ('t', TXT), 'label': ('l', LABEL), 'index': ('i', INDEX), 'ratio': ('r',RATIO),\n",
    "                                      'isolation':('iso', ISO), 'low':('low', LOW), 'deep': ('deep', DEEP)}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "TXT.build_vocab(train_data, vectors = \"glove.6B.100d\", unk_init = torch.Tensor.normal_)\n",
    "\n",
    "INDEX.build_vocab(train_data)\n",
    "RATIO.build_vocab(train_data)\n",
    "ISO.build_vocab(train_data)\n",
    "LOW.build_vocab(train_data)\n",
    "DEEP.build_vocab(train_data)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_iterator, dev_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, dev_data,test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_key = lambda x: len(x.t),\n",
    "    #sort = False,\n",
    "    #sort_within_batch = False,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
    "                 bidirectional, dropout, mix_layer, extra_layer,pad_idx):   \n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        self.rnn = nn.LSTM(embedding_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        #self.fc = nn.Linear((hidden_dim * 2), output_dim)\n",
    "        self.fc = nn.Linear((hidden_dim * 2)+3, mix_layer)\n",
    "        self.fc2 = nn.Linear(mix_layer, extra_layer)\n",
    "        self.fc3 = nn.Linear(extra_layer, output_dim)\n",
    "        \n",
    "    def forward(self, text, text_lengths, extra):\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.to('cpu'), enforce_sorted=False) \n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "        #final = self.fc(hidden)#.squeeze(-1))\n",
    "        #print(hidden.squeeze(1).shape, extra.unsqueeze(1).shape)\n",
    "        extras = self.fc(torch.cat((hidden.squeeze(1),extra),dim=1))\n",
    "        line = F.relu(self.fc2(extras))\n",
    "        final = self.fc3(line)\n",
    "            \n",
    "        return final\n",
    "\n",
    "\n",
    "model = RNN(vocab_size = len(TXT.vocab), \n",
    "            embedding_dim = 100, \n",
    "            hidden_dim = 256, \n",
    "            output_dim = 1,\n",
    "            mix_layer = 256,\n",
    "            extra_layer = 128, \n",
    "            n_layers = 2, \n",
    "            bidirectional = True, \n",
    "            dropout = 0.5, \n",
    "            pad_idx = TXT.vocab.stoi[TXT.pad_token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "def f1_scoring(preds, y):\n",
    "    from sklearn.metrics import f1_score\n",
    "    import numpy as np\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    score = f1_score(y.tolist(), rounded_preds.tolist())\n",
    "    return torch.from_numpy(np.array(score))\n",
    "    \n",
    "\n",
    "def train(model, iterator, optimizer, criterion): \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        text, text_lengths = batch.t\n",
    "        extra_feat = torch.cat((batch.low.unsqueeze(1), batch.deep.unsqueeze(1), batch.iso.unsqueeze(1)),dim=1)\n",
    "        predictions = model(text, text_lengths, extra_feat).squeeze(1)\n",
    "        loss = criterion(predictions, batch.l.float())\n",
    "        #acc = binary_accuracy(predictions, batch.l)\n",
    "        acc = f1_scoring(predictions, batch.l)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0 \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, text_lengths = batch.t\n",
    "            extra_feat = torch.cat((batch.low.unsqueeze(1), batch.deep.unsqueeze(1), batch.iso.unsqueeze(1)),dim=1)\n",
    "            predictions = model(text, text_lengths, extra_feat).squeeze(1)\n",
    "            loss = criterion(predictions, batch.l.float())\n",
    "            #acc = binary_accuracy(predictions, batch.l)\n",
    "            acc = f1_scoring(predictions, batch.l)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel.eval()\\nwith torch.no_grad():\\n    for b in train_iterator:\\n        #r = b.r\\n        #tag= b.tag\\n        #link = b.link\\n        #fav = b.fav\\n        txt, len_txt = b.t\\n        #ex = torch.cat((b.r, b.tag, b.link),-1)\\n        #text, text_lengths = b.t\\n        print(len_txt < 0)\\n        #predictions = model(text, text_lengths, b.r)\\n        #print(fav)\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for b in train_iterator:\n",
    "        #r = b.r\n",
    "        #tag= b.tag\n",
    "        #link = b.link\n",
    "        #fav = b.fav\n",
    "        txt, len_txt = b.t\n",
    "        #ex = torch.cat((b.r, b.tag, b.link),-1)\n",
    "        #text, text_lengths = b.t\n",
    "        print(len_txt < 0)\n",
    "        #predictions = model(text, text_lengths, b.r)\n",
    "        #print(fav)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.062 | Train Acc: 95.52%\n",
      "\t Val. Loss: 0.626 |  Val. Acc: 75.77%\n",
      "Epoch:  2 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.055 | Train Acc: 97.01%\n",
      "\t Val. Loss: 0.664 |  Val. Acc: 79.07%\n",
      "Epoch:  3 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.052 | Train Acc: 96.40%\n",
      "\t Val. Loss: 0.631 |  Val. Acc: 77.74%\n",
      "Epoch:  4 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.044 | Train Acc: 97.69%\n",
      "\t Val. Loss: 1.118 |  Val. Acc: 71.09%\n",
      "Epoch:  5 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.041 | Train Acc: 97.08%\n",
      "\t Val. Loss: 0.980 |  Val. Acc: 72.77%\n",
      "Epoch:  6 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.030 | Train Acc: 98.50%\n",
      "\t Val. Loss: 0.927 |  Val. Acc: 76.87%\n",
      "Epoch:  7 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.038 | Train Acc: 97.70%\n",
      "\t Val. Loss: 0.708 |  Val. Acc: 76.77%\n",
      "Epoch:  8 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.045 | Train Acc: 97.13%\n",
      "\t Val. Loss: 0.856 |  Val. Acc: 78.19%\n",
      "Epoch:  9 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.032 | Train Acc: 98.10%\n",
      "\t Val. Loss: 0.778 |  Val. Acc: 75.27%\n",
      "Epoch: 10 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.033 | Train Acc: 98.31%\n",
      "\t Val. Loss: 1.086 |  Val. Acc: 74.41%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "\n",
    "best_test_f1 = float('-inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_f1 = train(model, train_iterator, optimizer, criterion)\n",
    "    test_loss, test_f1 = evaluate(model, dev_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if test_f1 > best_test_f1:\n",
    "        best_test_f1 = test_f1\n",
    "        torch.save(model.state_dict(), 'model_LSTM_BI2_pool.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:2} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_f1*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {test_loss:.3f} |  Val. Acc: {test_f1*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT text representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m\n",
       "\u001b[0mtransformDF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mtokenize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mconcatenation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mconcat_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'simple'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mn_tweets\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0morder_by_deep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Transform and extract tweets data to a pandas dataframe, ready to export to json.\n",
       "Takes the original Dataframe and the list of labels as arguments.\n",
       "\n",
       "Arguments: df, label, tokenize=True, concat_type = 'simple', n_tweets = -1, order_by_deep = False\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\framo\\google drive\\0 mit\\0sem 1 2021\\nlp\\project\\<ipython-input-16-8a5e97cd9cf5>\n",
       "\u001b[1;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transformDF?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = transformDF(df, y_train.lab, tokenize=False, concatenation=False)\n",
    "devel = transformDF(dev, y_dev.lab,tokenize= False, concatenation=False)\n",
    "test1 = transformDF(test, test.index, tokenize=False, concatenation=False)\n",
    "\n",
    "train1.index = [i for i in range(train1.shape[0])]\n",
    "devel.index = [i for i in range(devel.shape[0])]\n",
    "test1.index = [i for i in range(test1.shape[0])]\n",
    "\n",
    "train1.to_json('trainC.json', orient = 'records', lines = True)\n",
    "devel.to_json('devC.json', orient = 'records', lines = True)\n",
    "test1.to_json('testC.json', orient = 'records', lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "class SSTDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df, maxlen=256):\n",
    "\n",
    "        #Store the contents of the file in a pandas dataframe\n",
    "        self.df = df\n",
    "\n",
    "        #Initialize the BERT tokenizer\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        #Selecting the sentence and label at the specified index in the data frame\n",
    "        sentence = self.df.loc[index, 'text']\n",
    "        label = self.df.loc[index, 'label']\n",
    "        indice = self.df.loc[index, 'index']\n",
    "\n",
    "        #Preprocessing the text to be suitable for BERT\n",
    "        tokens = self.tokenizer.tokenize(sentence) #Tokenize the sentence\n",
    "        tokens = ['[CLS]'] + tokens + ['[SEP]'] #Insering the CLS and SEP token in the beginning and end of the sentence\n",
    "        if len(tokens) < self.maxlen:\n",
    "            tokens = tokens + ['[PAD]' for _ in range(self.maxlen - len(tokens))] #Padding sentences\n",
    "        else:\n",
    "            tokens = tokens[:self.maxlen-1] + ['[SEP]'] #Prunning the list to be of specified max length\n",
    "\n",
    "        tokens_ids = self.tokenizer.convert_tokens_to_ids(tokens) #Obtaining the indices of the tokens in the BERT Vocabulary\n",
    "        tokens_ids_tensor = torch.tensor(tokens_ids) #Converting the list to a pytorch tensor\n",
    "\n",
    "        #Obtaining the attention mask i.e a tensor containing 1s for no padded tokens and 0s for padded ones\n",
    "        attn_mask = (tokens_ids_tensor != 0).long()\n",
    "\n",
    "        return tokens_ids_tensor, attn_mask, label, indice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done preprocessing training and development data.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#Creating instances of training and development set\n",
    "#maxlen sets the maximum length a sentence can have\n",
    "#any sentence longer than this length is truncated to the maxlen size\n",
    "train_set = SSTDataset(train1)\n",
    "dev_set = SSTDataset(devel)\n",
    "test_set = SSTDataset(test1)\n",
    "#Creating intsances of training and development dataloaders\n",
    "train_loader = DataLoader(train_set, batch_size = 10)\n",
    "dev_loader = DataLoader(dev_set, batch_size = 10)\n",
    "test_loader = DataLoader(test_set, batch_size = 10)\n",
    "\n",
    "print(\"Done preprocessing training and development data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bert tutorial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "class SentimentClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        #Instantiating BERT model object \n",
    "        self.bert_layer = BertModel.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        #Classification layer\n",
    "        #input dimension is 768 because [CLS] embedding has a dimension of 768\n",
    "        #output dimension is 1 because we're working with a binary classification problem\n",
    "        self.cls_layer = nn.Linear(768, 1)\n",
    "\n",
    "    def forward(self, seq, attn_masks):\n",
    "        '''\n",
    "        Inputs:\n",
    "            -seq : Tensor of shape [B, T] containing token ids of sequences\n",
    "            -attn_masks : Tensor of shape [B, T] containing attention masks to be used to avoid contibution of PAD tokens\n",
    "        '''\n",
    "\n",
    "        #Feeding the input to BERT model to obtain contextualized representations\n",
    "        outputs = self.bert_layer(seq, attention_mask = attn_masks)\n",
    "        cont_reps = outputs.last_hidden_state\n",
    "\n",
    "        #Obtaining the representation of [CLS] head (the first token)\n",
    "        cls_rep = cont_reps[:, 0]\n",
    "\n",
    "        #Feeding cls_rep to the classifier layer\n",
    "        logits = self.cls_layer(cls_rep)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BERTGRU(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_dim,\n",
    "                 output_dim,\n",
    "                 n_layers,\n",
    "                 bidirectional,\n",
    "                 dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        embedding_dim = bert.config.to_dict()['hidden_size']\n",
    "        self.rnn = nn.GRU(embedding_dim,\n",
    "                          hidden_dim,\n",
    "                          num_layers = n_layers,\n",
    "                          bidirectional = bidirectional,\n",
    "                          batch_first = True,\n",
    "                          dropout = 0 if n_layers < 2 else dropout)\n",
    "        \n",
    "        self.out = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text, attn_masks):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            embedded = self.bert(text, attn_masks)[0]\n",
    "        _, hidden = self.rnn(embedded)\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "        output = self.out(hidden)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "bert = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERTGRU(bert,\n",
    "             hidden_dim = 256,\n",
    "             output_dim = 1,\n",
    "             n_layers = 2,\n",
    "             bidirectional= True,\n",
    "             dropout = 0.25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "device = 'cuda:0'\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "def train(model, iterator, optimizer, criterion): \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        seq, attn_masks, labels = batch\n",
    "        labels = labels.to(device)\n",
    "        predictions = model(seq.to(device), attn_masks.to(device)).squeeze(1)\n",
    "        loss = criterion(predictions, labels.float())\n",
    "        #acc = binary_accuracy(predictions, batch.l)\n",
    "        acc = f1_scoring(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0 \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            seq, attn_masks, labels = batch\n",
    "            labels = labels.to(device)\n",
    "            predictions = model(seq.to(device), attn_masks.to(device)).squeeze(1)\n",
    "            loss = criterion(predictions, labels.float())\n",
    "            #acc = binary_accuracy(predictions, batch.l)\n",
    "            acc = f1_scoring(predictions, labels)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 | Epoch Time: 5m 37s\n",
      "\tTrain Loss: 0.075 | Train Acc: 93.64%\n",
      "\t Val. Loss: 0.474 |  Val. Acc: 79.64%\n",
      "Epoch:  2 | Epoch Time: 5m 36s\n",
      "\tTrain Loss: 0.059 | Train Acc: 94.77%\n",
      "\t Val. Loss: 0.583 |  Val. Acc: 79.57%\n",
      "Epoch:  3 | Epoch Time: 5m 37s\n",
      "\tTrain Loss: 0.050 | Train Acc: 95.03%\n",
      "\t Val. Loss: 0.620 |  Val. Acc: 76.77%\n",
      "Epoch:  4 | Epoch Time: 5m 36s\n",
      "\tTrain Loss: 0.046 | Train Acc: 95.14%\n",
      "\t Val. Loss: 0.572 |  Val. Acc: 74.93%\n",
      "Epoch:  5 | Epoch Time: 5m 37s\n",
      "\tTrain Loss: 0.046 | Train Acc: 95.17%\n",
      "\t Val. Loss: 0.690 |  Val. Acc: 78.05%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "\n",
    "best_test_f1 = float('-inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_f1 = train(model, train_loader, optimizer, criterion)\n",
    "    test_loss, test_f1 = evaluate(model, dev_loader, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if test_f1 > best_test_f1:\n",
    "        best_test_f1 = test_f1\n",
    "        torch.save(model.state_dict(), 'model_BERTGRU.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:2} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_f1*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {test_loss:.3f} |  Val. Acc: {test_f1*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, iterator):\n",
    "    model.eval()\n",
    "    predict = []\n",
    "    index = []\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, text_lengths = batch.t\n",
    "            extra_feat = torch.cat((batch.low.unsqueeze(1), batch.deep.unsqueeze(1), batch.iso.unsqueeze(1)),dim=1)\n",
    "            preds = model(text, text_lengths, extra_feat).squeeze(1)\n",
    "            rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "            predict = predict + rounded_preds.tolist()\n",
    "            index = index + batch.i.tolist()\n",
    "    \n",
    "    result = {x:y for x,y in zip(index, predict)}\n",
    "    return result   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.load_state_dict(torch.load('model_LSTM_BI2.pt'))\n",
    "\n",
    "#dict_p = predict(model, test_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_p = predict(model, test_iterator)\n",
    "out = {k:'rumour' if v>0 else 'non-rumour' for k,v in dict_p.items()}\n",
    "import json\n",
    "with open('test-output.json', 'w') as fp:\n",
    "    json.dump(out, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "out = {k:'non-rumour' for k in devel['index']}\n",
    "with open('dev.result.json', 'w') as fp:\n",
    "    json.dump(out, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "dict_p1 = predict(model, dev_iterator)\n",
    "out1 = {k:'rumour' if v>0 else 'non-rumour' for k,v in dict_p1.items()}\n",
    "with open('dev.result.json', 'w') as fp:\n",
    "    json.dump(out1, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('544458445764968448', '580338194731962368', '552825844755083265', '552807904597008385', '552804592988479488', '525025279803424768', '552784600502915072', '499696525808001024', '580320612155060224', '553218279557582849')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for batch in dev_loader:\n",
    "        seq, attn_masks, labels, indice = batch\n",
    "print(indice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, iterator):\n",
    "    model.eval()\n",
    "    predict = []\n",
    "    index = []\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            seq, attn_masks, labels, indice = batch\n",
    "            labels = labels.to(device)\n",
    "            preds = model(seq.to(device), attn_masks.to(device)).squeeze(1)\n",
    "            rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "            predict = predict + rounded_preds.tolist()\n",
    "            index = index + list(indice)\n",
    "    \n",
    "    result = {x:y for x,y in zip(index, predict)}\n",
    "    return result\n",
    "\n",
    "model.load_state_dict(torch.load('model_BERTGRU.pt'))\n",
    "dict_p1 = predict(model, dev_loader)\n",
    "out1 = {k:'rumour' if v>0 else 'non-rumour' for k,v in dict_p1.items()}\n",
    "with open('dev.result.json', 'w') as fp:\n",
    "    json.dump(out1, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
